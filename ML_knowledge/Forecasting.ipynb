{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4087e6d-dafd-4612-ac41-6af140a62e2b",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "- Statistical and machine learning models on time-series data\n",
    "- Credits: DSCI 574 Spatial & Temporal Models, Quan Nguyen, Feb 2022\n",
    "- Detailed local version: [Note 1](http://localhost:8888/doc/tree/mds/Block5/simon-block5/574-Note-1.ipynb) and [Note 2](http://localhost:8888/doc/tree/mds/Block5/simon-block5/574-Note-2.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bb9bc8-2a54-40ba-84f5-de401717fd67",
   "metadata": {},
   "source": [
    "## Intro to Time-Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083b8ce5-3d68-4266-abbb-a72e0de96a53",
   "metadata": {},
   "source": [
    "### What is time series?\n",
    "\n",
    "- A time series is a collection of observations recorded sequentially in time\n",
    "    - For the statistically inclined, we define a time series as a collection of random variables indexed by time\n",
    "        - For example, consider the sequence of random variables, $y_1, y_2, y_3$ etc., where the random variable $y_i$ denotes the value of the times series at the $i$th time point.\n",
    "    - In general, a collection of random variables, $y_t$ indexed by $t$ is referred to as a *stochastic process*.\n",
    "- Observations in a series may be evenly spaced (**regular time series**) or unevenly space (**irregular time series**)\n",
    "    - We will be focusing on regular time series in this course\n",
    "    - If you encounter an irregular time series, typically you could aggregate it to a regular interval, and/or to impute missing values\n",
    "- Generally there are two main things we want to do with a time series:\n",
    "    1. Explanatory modelling to **understand the past**\n",
    "    2. Predictive modelling to **forecast the future**\n",
    "    - (In this section we will be focusing on the explanatory modelling to understand the past)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359a2fde-6e4a-45d1-8ff4-def874499b0e",
   "metadata": {},
   "source": [
    "### Time Series Features\n",
    "\n",
    "Visualization and temporal dependency:\n",
    "\n",
    "- The key difference between other data and time series data: temporal dependency!\n",
    "    - We can quantify this dependency by looking at the correlation of a time series with \"lagged\" values of itself. \n",
    "        - We call this **autocorrelation**\n",
    "    - We can easily \"lag\" a time series in Pandas with the `.shift()` method: \n",
    "        - `df[\"time (lag=1)\"] = df[\"time\"].shift(1)`\n",
    "        - Correlation between the two: `df[\"time\"].corr(df[\"time (lag=1)\"])`\n",
    "- A **correlogram** plots the autocorrelation function (ACF) on the y-axis and lags on the x-axis \n",
    "    - (we call it the autocoreelation *function* because it is a function of lag)\n",
    "    - useful package and functions: `from statsmodels.graphics.tsaplots import plot_acf`\n",
    "- We'll explore this notion of trends more later, but for now, some key observations about the correlograms:\n",
    "    - The ACF will almost always decay with the lag (observations farther apart in time are less correlated)\n",
    "    - If a series alternates (i.e., consecutive values tend to be on the opposite sides of the mean, like our sunspots data), then the ACF alternates too.\n",
    "    - If a series has seasonal or cyclical fluctuations, the ACF will oscillate at the same frequency.\n",
    "    - If the series has a trend, the ACF will have a very slow decay due to high correlation of the consecutive values (which tend to lie on the same side of the mean)\n",
    "    - In general, experience is required to glean much from an ACF plot. We will use the correlogram as a model selection tool later in the course.\n",
    "    \n",
    "Time Series Pattern:\n",
    "- There are 3 main patterns of a time series you should be aware of:\n",
    "    1. **Trend**: long term increases or decreases in the series.\n",
    "    2. **Seasonality**: regular variation in the series at some fixed interval, e.g., month, day of week, time of day, etc.\n",
    "    3. **Cyclicity**: variations in the series that repeat with some regularity but of unknown and changing period.\n",
    "\n",
    "White noise:\n",
    "- Time series that show no autocorrelation with zero mean and constant variance are called **white noise**\n",
    "    - often we assume that the white noise is iid and Gaussian distributed denoted as $w_t\\sim\\mathcal{N}(0,\\sigma^2)$\n",
    "- Think of white noise as completely uninteresting with no predictable patterns\n",
    "    - if our series is white noise, it means it is a series of random numbers and cannot be predicted\n",
    "    - white noise can also help us to check whether there is information/dependency in our time series that we can model\n",
    "        - (as we will see in the next section)\n",
    "- As a result, we expect the ACF of a white noise series to be close to 0 (no correlation) for all lags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fab90b9-7a13-4b62-aa87-900bb0047a9d",
   "metadata": {},
   "source": [
    "### Time Series Decomposition\n",
    "\n",
    "- When we decompose a time series, we usually split it into 3 components:\n",
    "    1. **Trend-cycle** ($T$)\n",
    "        - Comparing to seasonal component, trend component is a long term change in the mean of the series, whereas the seasonality is a regular, repeating variation that repeats at known periods\n",
    "        - **Curve-fitting and moving average**\n",
    "    2. **Seasonal** ($S$)\n",
    "        - The difference between \"seasonality\" and \"cyclicity\" in a time series is that\n",
    "            - Seasonality is a regular, repeating variation that repeats at known periods\n",
    "            - Cyclicity is a repeating variation of varying period and magnitude\n",
    "        - Subtracting/Dividing the trend-cycle effect(i.e. detrending) first then take average\n",
    "    3. **Remainder** ($R$) (also called the \"residual\")\n",
    "        - Calculate by following the formulae below (use observation to subtract or divide $S$ and $T$)\n",
    "- There are two main ways we can combine/decompose these components to make up a time series:\n",
    "    1. **Additive**: $y_t = S_t + T_t + R_t$. \n",
    "        - Appropriate if the magnitude of the seasonal fluctuations, or the variation around the trend-cycle, does not vary with the value of the series.\n",
    "    2. **Multiplicative**: $y_t = S_t \\times T_t \\times R_t$. \n",
    "        - Appropriate if variation in the seasonal pattern, or the variation around the trend-cycle, appears to be proportional (i.e. variation increases as the time goes by) to the value of series.\n",
    "    - Usually we would use additive method since multiplicative one has a relatively strong assumption on the proportional relationship between time and the variation in the seasonal pattern.\n",
    "- `statsmodels.tsa` has many great tools that handle decomposition for us\n",
    "- Probably the most common is an STL decomposition which uses Loess (kind of like kNN and OLS regression had a baby)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e99a5f-8889-4910-ace0-ec43fe83d1e7",
   "metadata": {},
   "source": [
    "## Intro to Forecasting\n",
    "\n",
    "- Unlike predicting, forecasting means that we are predicting values in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36efb024-83fc-4229-ac0d-b9392eea2ecc",
   "metadata": {},
   "source": [
    "### Baseline methods\n",
    "\n",
    "Before we get into each method, some notations:\n",
    "- $y_t$: value of a time series at time $t$\n",
    "- $h$: a forecast horizon\n",
    "    - i.e. $h=1$ means we want to predict one time step ahead\n",
    "- $T$: length of a time series\n",
    "- $\\hat{y}$: forecast value\n",
    "- $y$: observed value\n",
    "- $\\hat{y}_{t|t-1}$: the value of $\\hat{y}_t$ given $y_{t-1}$\n",
    "\n",
    "Now the baseline methods:\n",
    "- **Average**\n",
    "    - Use the average of the series for all future forecasts\n",
    "    - i.e. mathematically $\\hat{y}_{T+h}=\\bar{y}$ for all $h$\n",
    "- **Naive**\n",
    "    - Use the last observation for all forecasts\n",
    "    - i.e. $\\hat{y}_{T+h|T}=y_T$ for all $h$\n",
    "- **Seasonally-adjusted naive**\n",
    "    - Similar to naive method, but the data are seasonally adjusted by applying a classical decomposition\n",
    "    - i.e. $\\hat{y}_{T+h|T}=y'_T$ for all $h$ where $y'$ stands for the transformed seasonally adjusted data\n",
    "        - e.g. if we are using multiplicative decomposition, then $y' = \\frac{y}{\\text{model.seasonal}}$\n",
    "- **Seasonal naive**\n",
    "    - Set each forecast as the last observed value from the same season of the year (e.g. the same month of the previous year)\n",
    "    - More specifically for our monthly data, the forecasts for all future January is the last observed January value\n",
    "- **Drift**\n",
    "    - Forecasts equal to last value in the series plus the average (global, not step-wise) change of the series\n",
    "    - i.e. $\\hat{y}_{T+h|T}=y_T+h\\left(\\frac{y_T-y_1}{T-1}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada0b55-3d3b-4ac3-b564-a7a474cf422b",
   "metadata": {},
   "source": [
    "### Exponential models\n",
    "\n",
    "- In Simple Exponential Smoothing(**SES**), our forecast is an exponentially weighted average of past values:\n",
    "$$\\hat{y}_{t+1} = \\alpha{}y_t + \\alpha{}(1 - \\alpha{})y_{t-1} + \\alpha{}(1 - \\alpha{})^2y_{t-2} + \\cdots$$\n",
    "    - where $0\\le\\alpha{}\\le1$ and $\\hat{y}$ refers to a forecasted value\n",
    "    - We can re-write that in the recursive form:\n",
    "$$\\hat{y}_{t+1|t} = \\alpha{}y_t + (1-\\alpha{})\\hat{y}_{t|t-1}$$\n",
    "    - (TODO)\n",
    "- In **Holt's method**, we extend smoothing based on the SES:\n",
    "$$\\hat{y}_{t+h|t}=\\ell_t+hb_t$$\n",
    "\n",
    "$$\\ell_t=\\alpha{}y_t+(1-\\alpha{})(\\ell_{t-1}+b_{t-1})$$\n",
    "\n",
    "$$b_t=\\beta(\\ell_t-\\ell_{t-1})+(1-\\beta)b_{t-1}$$\n",
    "    - We now have two key parameters $\\alpha$ (to control smoothness of the level) and $\\beta$ (control smoothness of trend). Read more about it [here](https://otexts.com/fpp3/holt.html) if you wish.\n",
    "    - All we're doing here is forecasting the next values as an exponentially weighted average of past values and past trend\n",
    "- **Holt-Winter's method** extends even further:\n",
    "$$\\hat{y}_{t+h|t}=\\ell_t+hb_t+s_{t+h-m(k+1)}$$\n",
    "\n",
    "$$\\ell_t=\\alpha{}(y_t-s_{t-m})+(1-\\alpha{})(\\ell_{t-1}+b_{t-1})$$\n",
    "\n",
    "$$b_t=\\beta(\\ell_t-\\ell_{t-1})+(1-\\beta)b_{t-1}$$\n",
    "\n",
    "$$\\text{additive: }s_t=\\gamma(y_t-\\ell_{t-1}-b_{t-1})+(1-\\gamma)s_{t-m}$$ \n",
    "$$\\text{multiplicative: }s_t=\\gamma\\frac{y_t}{\\ell_{t-1}+b_{t-1}}+(1-\\gamma)s_{t-m}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02607c7-8531-4db2-8191-edf92b8ceee9",
   "metadata": {},
   "source": [
    "### ETS models\n",
    "- The methods above providing point forecasts, but what if we want to have prediction intervals?\n",
    "- The generalization of exponential smoothing algorithms to statistical models that model distributions, include an error term, and can generate prediction intervals are known as **ETS** models (**E**rror, **T**rend, **S**easonal), where:\n",
    "    - E = {additive, multiplicative}\n",
    "    - T = {none, additive, additive damped}\n",
    "    - S = {none, additive, multiplicative}\n",
    "- You can read more about ETS models and their derivation [here](https://otexts.com/fpp3/ets.html) and in [Appendix B](https://pages.github.ubc.ca/MDS-2021-22/DSCI_574_spat-temp-mod_students/lectures/appendixB_state-space-models.html) but their derivation is not really important to know and beyond the scope of this course.\n",
    "    - However do note that an ETS model will usually give the same/similar results to the algorithms above, but with the added bonus of being able to generate prediction intervals\n",
    "    - In Rob Hyndman's [own words](https://robjhyndman.com/hyndsight/estimation2/) *\"the results from ETS are usually more reliable (than the algorithmic exponential models)\"*\n",
    "- Rather than optimizing based on minimizing the SSE, ETS models optimize by maximizing the likelihood (we assume errors are normally distributed).\n",
    "    - For more details on that also see [Appendix B](https://pages.github.ubc.ca/MDS-2021-22/DSCI_574_spat-temp-mod_students/lectures/appendixB_state-space-models.html)\n",
    "|Trend Component|Seasonal Component|\n",
    "|---|---|\n",
    "|None `(N)`|None `(N)`|\n",
    "|Additive `(A)`|Additive `(A)`|\n",
    "|Additive damped `(Ad)`|Multiplicative `(M)`|\n",
    "\n",
    "|Notation|Method|\n",
    "|---|---|\n",
    "|`(N,N)`|Simple exponential smoothing|\n",
    "|`(A,N)`|Holt's method|\n",
    "|`(A,A)`|Additive Holt-Winter's method|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007a3dc-c2ab-446a-91d8-25b9d1cc4480",
   "metadata": {},
   "source": [
    "### Selecting a model\n",
    "\n",
    "#### In-sample methods\n",
    "- Metrics\n",
    "    - The most common metrics are:\n",
    "        - Akaike information criterion(AIC)\n",
    "        - Bayesian information criterion(BIC)\n",
    "        - ![](../images/aic_bic.png)\n",
    "        - Sum of Squared Errors (SES)/Mean Squared Errors (MSE)/Root Mean Squared Errors(RMSE)\n",
    "    - We can extract them from most models in `statsmodels` with `model.summary()` or extract with `model.aic/model.bic/model.mse` \n",
    "- Residuals\n",
    "    - We can use residuals to reflect how well our model captures information in the data by\n",
    "        1. Visual inspection (residuals are uncorrelated, have zero mean, and ideally normally distributed)\n",
    "            - We can use `plot_acf` on `model.resid` and see if residuals are significantly different from white noise\n",
    "            - If it has structure and different from noise, then it is not good\n",
    "        2. Running diagnostic Portmanteau tests (e.g. Ljung-Box-Perce test, etc.)\n",
    "            - The *[Ljung–Box test](https://en.wikipedia.org/wiki/Ljung%E2%80%93Box_test)* tests whether a group of autocorrelations is significantly different from white noise\n",
    "            - The *[Jarque-Bera test](https://en.wikipedia.org/wiki/Jarque%E2%80%93Bera_test)* tests whether residuals are significantly different from a normal distribution (based on skewness and kurtosis)\n",
    "            - `statsmodels` provides the test statistics for us with `model.test_serial_correlation('ljungbox', lags=24)` and `model.test_normality('jarquebera')`\n",
    "    - However these tests tend to be not that useful in practice... but could consider them since they are included in the most packages anyway\n",
    "- See examples with code details on the [course note section](https://pages.github.ubc.ca/MDS-2021-22/DSCI_574_spat-temp-mod_students/lectures/lecture2_intro-to-forecasting.html#in-sample-methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2d050-4c52-4c3a-85b8-18d41e0c54a1",
   "metadata": {},
   "source": [
    "#### Out-of-sample methods\n",
    "- Usually we are interested in using models to forecast, hence we care about its performance on unseen data\n",
    "- Typically method would be having a training set and a validation set\n",
    "- To measure the performance of model forecasts, the most common regression metrics are:\n",
    "    1. Mean Absolute Error (MAE): $\\frac{1}{n}\\sum^{n}_{i=1}|y_i - \\hat{y}_i|$\n",
    "    2. Root Mean Squared Error (RMSE): $\\sqrt{\\frac{1}{n}\\sum^{n}_{i=1}(y_i - \\hat{y}_i)^2}$\n",
    "    3. Mean Absolute Percentage Error (MAPE): $\\frac{1}{n}\\sum^{n}_{i=1}|\\frac{y_i - \\hat{y}_i}{y_i}|$\n",
    "    4. Mean Absolute Scaled Error (MASE): $\\frac{MAE}{\\frac{1}{T-1}\\sum^{T}_{t=2}|y_t-y_{t-1}|}$\n",
    "- Some notes on these metrics:\n",
    "    - MAE and RMSE are popular in practice because they are easier to interpret. Closer to 0 is better.\n",
    "    - MAPE is scale-free and aims to proportionalize errors, such that the error for $\\hat{y}=12$ and $y=10$ (MAPE = 20%, MSE = 4) is the same as $\\hat{y}=120$ and $y=100$ (MAPE = 20%, MSE = 400).  Closer to 0 is better. \n",
    "        - MAPE is problematic if 0 values are expected (divide by 0 error) and it is also not symmetrical, i.e., $\\hat{y}=150$ and $y=100$ gives $MAPE=\\frac{|100-150|}{100}=33.33\\%$, but $\\hat{y}=100$ and $y=150$ gives $MAPE=\\frac{|150-100|}{150}=50\\%$. \n",
    "        - There is a version, `sMAPE` available that is symmetrical, but MASE is often preferred.\n",
    "    - MASE scales the MAE based on the MAE of a naive forecast on the training data. I think of this as the \"r-squared\" of the forecasting world. It corrects the above errors. Value < 1 indicate forecasts are better than in-sample naive forecasts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a4a0d-b07b-4931-b09c-5d47c1522d6d",
   "metadata": {},
   "source": [
    "## ARIMA Models\n",
    "\n",
    "(TODO)\n",
    "\n",
    "### Stationary\n",
    "- A stationary time series is one whose properties do not depend on time\n",
    "    - Is roughly horizontal\n",
    "    - Has a constant mean & variance\n",
    "    - Does not show predictable patterns (e.g., seasonality)\n",
    "    - Note that a time series can be non-stationary even if it has no trend\n",
    "        - The expected value of a series may depend on time as a result of seasonality and changing variance\n",
    "    - On the other than, a time series can be stationary even if it has non-zero autocorrelation for some lags higher than 0\n",
    "        - A series can be stationary, yet autocorrelation can still arise when observations are influenced by previous observations; consider a stationary AR(1) process as an example\n",
    "\n",
    "AR: AutoRegressive model\n",
    "\n",
    "MA: Moving Average\n",
    "\n",
    "ARMA: Autoregression + moving average\n",
    "\n",
    "ARIMA: Autoregressive Integrated Moving Average\n",
    "\n",
    "Seasonal ARIMA (SARIMA) and adding explanatory to the ARMA (SARIMAX)\n",
    "\n",
    "Choosing order:\n",
    "- Picking `p` for the AR component\n",
    "    - Recall that a corellogram (ACF plot) shows autocorrelations at different lags\n",
    "        - however ACF only tells us how correlated it is between $y_t$ and $y_{t-h}$\n",
    "            - The problem is that if $y_t$ and $y_{t-1}$ are correlated, then $y_{t-1}$ and $y_{t-2}$ are also correlated, and therefore, $y_t$ and $y_{t-2}$ have indirect correlation (via $y_{t-1}$).\n",
    "            - This makes it hard to isolate exactly which lags are important in our series. We can clearly see this in the model above which should only show one significant lag.\n",
    "    - The solution to the problem with ACF is **partial autocorrelation** (PACF) which removes intermediate effects between $y_t$ and $y_{t-h}$. \n",
    "        - The partial autocorrelation is estimated as the last coefficient in an autoregressive model. So $PACF(k)$ is the $k$th estimated coefficient in an `AR(k)` model\n",
    "    - By look at the correllogram with PACF, we can pick the `p` (the order of AR model) where `p` equals to the number of lags it takes to decrease down to the confidence interval range\n",
    "        - we want to see how many lags are heavily correlated\n",
    "- Picking `q` for the MA component\n",
    "    - Recall that `MA(q)` models are based on white noise. \n",
    "    - As we saw before, the value of an `MA(q)` model at time $t$ is a weighted sum of the last `q` values of the white noise process. \n",
    "    - Since there is no dependence structure on values at lags higher than `q`, it's just white noise, so we would expect the ACF to \"cut off\" at lag `q`, and pick that `q`\n",
    "- See [coded example](https://pages.github.ubc.ca/MDS-2021-22/DSCI_574_spat-temp-mod_students/lectures/lecture3_arima-models.html#choosing-orders) in the course notes.\n",
    "    - Notice that in the sliding bar example with phi (the correlation coefficient $\\phi$), when we have high phi, we tend to have larger correlation and we can see graduate decrease in the ACF and quick reduce in PACF with some bouncing around the zero line\n",
    "\n",
    "|Model|ACF|PACF|\n",
    "|---|---|---|\n",
    "|MA(q)|Cuts-off at lag `q`|Tails off, no pattern|\n",
    "|AR(p)|Tails off (exponentially or like a \"damped\" sine wave)|Cuts-off at lag p|\n",
    "\n",
    "- Auto-arima\n",
    "- Box-Jenkins\n",
    "![](../images/box_jenkins.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa919983-bda1-49b9-9555-232b20cd0584",
   "metadata": {},
   "source": [
    "## Forecasting with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba4c6fa-6a95-417c-8ded-873682a3c859",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c77124-c6e7-43fe-87ba-cb04b2477fa5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a5b2f80d-92ba-4c41-81f6-ee2815f6c2ef",
   "metadata": {},
   "source": [
    "## Other Forecasting Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca1e3a9-4ec3-49ae-897b-f9e49c120847",
   "metadata": {},
   "source": [
    "## Advanced Forecasting Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f30518e-5a81-4337-916a-f8bce7775a1a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55625328-2ef5-4021-a774-d5b4154bfa81",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a18243a-6fa4-4377-8320-45da6844d529",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ddf4919b-e261-4002-8496-ae0f3efc4e98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1beab48-7ce4-4679-953f-25eec0943e08",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "609a32bd-c08b-4a34-bfb2-d1d3e5e15b22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e64a9c6c-8e31-4381-b741-12b6261df0a7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dstools]",
   "language": "python",
   "name": "conda-env-dstools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
