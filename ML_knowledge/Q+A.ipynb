{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ede324f-23cf-4dcf-8153-eaa08c168f2d",
   "metadata": {},
   "source": [
    "# General Machine Learning Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a6359-430f-4fc5-95fc-dc11d193d214",
   "metadata": {},
   "source": [
    "## Collected Interview Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3d792-6ab0-424c-b894-e6713c9458f7",
   "metadata": {},
   "source": [
    "### What is Gradient Descent?\n",
    "\n",
    "- Gradient descent is an optimization algorithm that helps us to optimize our loss function; as the naming suggests, we are leveraging the gradient of our loss function to help us optimize our model parameters\n",
    "    - Mathematically speaking, gradient is just a vector of (partial) **derivatives of the loss function** w.r.t the model parameters\n",
    "- Gradients/derivatives tell us which direction (by sign) and by how much should be adjust the model parameters (by magnitude)\n",
    "    - Gradients point in the direction of steepest ascent, and we adjust the weights with the hyperparameter learning rate as $w_{new} = w_{old} - \\alpha \\nabla f$, where $\\alpha$ is the learning rate and $\\nabla f$ is the derivative/gradient\n",
    "\n",
    "#### Why do we need to scale before gradient descent?\n",
    "- When we apply gradient descent on multiple variables/weights, if their ranges and distributions are very different, then we would have very different step sizes for each feature.\n",
    "    - It makes the training faster\n",
    "    - It prevents the optimization from getting stuck in local optima\n",
    "    - It gives a better error surface shape\n",
    "    - Weight decay and Bayes optimization can be done more conveniently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c568a5-adf1-4997-82a4-0b77f1f0ee60",
   "metadata": {},
   "source": [
    "### What is Stochastic Gradient Descent (SGD)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23499cd-cf4b-4f72-84ff-b13981c44d8f",
   "metadata": {},
   "source": [
    "### Is it possible to apply GD or SGD with non-convex loss function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd04bd78-c586-40a2-bfe4-930a1f5e0102",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a83fe8-f187-47a5-87d7-46e2809715de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f8ec7de-1abf-413a-b31f-eadc4b6a2c40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1acd36c-fe16-44a7-9950-1b7a5f2640de",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb5e5a4a-525e-4f78-82de-419b154c0084",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8f4a316-9bf6-4593-992b-a9e0782a4a0e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc34d930-0b21-4cab-904d-1159bcc6b436",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "203bfba7-d72d-43ff-b752-27393b32bad6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45392fa7-6065-4cd2-a33b-a8bb016d3e91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37591264-c4ef-4fc8-810c-1c3be408c1ee",
   "metadata": {},
   "source": [
    "## Top ML Interview Questions\n",
    "\n",
    "Adapted from a post from https://career.guru99.com/.\n",
    "\n",
    "With the help of many other online resources, especially "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149bda83-98d0-47dd-984f-f5b49d75dacc",
   "metadata": {},
   "source": [
    "### What is Machine learning?\n",
    "\n",
    "The way I like to see it, machine learning(ML) is the baby of computer science and statistics, as it joins forces from both fields of study. Statistical learning acts as the twin (some also state statistical learning = machine learning + statistics, although that seems debatable to me) of machine learning since both are built on top of the statistical theories and apply system programming to learn the pattern implicitly included in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1084904-5aa0-4eef-9f9a-57771441cb12",
   "metadata": {},
   "source": [
    "### Mention the difference between Data Mining and Machine learning?\n",
    "\n",
    "Machine learning relates with the process of studying, designing and optimizing algorithms to give computers the capability to learn without being explicitly programmed. Whereas While, data mining is designed to extract the rules from large quantities of data and can be defined as the process in which the unstructured data tries to extract knowledge or unknown interesting patterns. Or to put it another way, data mining is simply a method of researching to determine a particular outcome based on the total of the gathered data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c5b948-ea79-48d5-a085-fb5ab64f49fd",
   "metadata": {},
   "source": [
    "### What is Over-fitting in Machine learning?\n",
    "\n",
    "- Famous bias-variance trade-off: with the increase of the complexity of the model, we would have decreasing trend for biases but the variances would increase instead;\n",
    "- Over-fitting happens when our variances are much higher than the biases (i.e. we focus too much on the variances), which means the model is tailored to the data being trained, which means the generality and stability of the model would be sacrificed\n",
    "- In terms of the training process, when the training error is going to be very low and there is a big gap between the training error and the validation error. This is overfitting.\n",
    "- Over-fitting exists because there exists noises in the data besides the actual pattern we are trying to capture with the machine learning algorithms and models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e5b66d-55ae-4b3a-aae0-fb1351daa871",
   "metadata": {},
   "source": [
    "### How to avoid overfitting?\n",
    "\n",
    "- Using cross-validation which is a technique of splitting the training data into folds and each fold acts as the validation set/\"fake\" testing set during each training and validating process.\n",
    "- Using a large amount of data, and more importantly, representative data with as few noises as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977f708e-f188-4e17-a70b-2ba3492be5cc",
   "metadata": {},
   "source": [
    "### Different kinds of machine learning?\n",
    "\n",
    "- There are perhaps 14 types of learning that you must be familiar with as a machine learning practitioner; they are:\n",
    "- Learning Problems\n",
    "    1. Supervised Learning\n",
    "    2. Unsupervised Learning\n",
    "    3. Reinforcement Learning\n",
    "        - a class of problems where an agent operates in an environment and must learn to operate using feedback\n",
    "- Hybrid Learning Problems\n",
    "    4. Semi-Supervised Learning\n",
    "    5. Self-Supervised Learning\n",
    "    6. Multi-Instance Learning\n",
    "- Statistical Inference\n",
    "    7. Inductive Learning\n",
    "    8. Deductive Inference\n",
    "    9. Transductive Learning\n",
    "- Learning Techniques\n",
    "    10. Multi-Task Learning\n",
    "    11. Active Learning\n",
    "    12. Online Learning\n",
    "    13. Transfer Learning\n",
    "    14. Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f78c06f-828e-4764-9b1d-3dfc3d81c4ae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08b1c6f7-7aef-4922-b9e3-d755a936e737",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db3da6c7-60cb-4aa3-bfcd-d0655d70986e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2662fb0-3d15-4ae3-ac9a-8d659ca6e9f7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e447eb0-3d86-487d-bd91-e5e98de70929",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20d1094c-066b-42da-89a2-083c2c86615a",
   "metadata": {},
   "source": [
    "# Questions Prepared for Interviews\n",
    "\n",
    "This is a set of questions that I prepared for interviews and my own learning/review purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285262db-3f28-4476-b430-4a0ed348cfcb",
   "metadata": {},
   "source": [
    "## What is...?\n",
    "\n",
    "Some \"what is\" questions -- mainly on the definition and quick explanations of different machine learning models/algorithms/techniques that people usually apply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cc6f8-213e-4111-a059-0397370d99ba",
   "metadata": {},
   "source": [
    "### What is linear regression?\n",
    "\n",
    "- $y = ax + b$\n",
    "- Regression I 561, Regression II 562"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbfd1d8-860f-4c99-87d7-d759acb3716b",
   "metadata": {},
   "source": [
    "### What is logistic regression?\n",
    "\n",
    "- $h(\\pi_i) = \\mbox{logit}(\\pi_i) = \\log \\bigg( \\frac{\\pi_i}{1 - \\pi_i}\\bigg) = \\beta_0 + \\beta_1 X_{i, 1} + \\beta_1 X_{i, 2} + \\ldots + \\beta_p X_{i, p}$\n",
    "- Equivalently: $\\pi_i = \\frac{\\exp\\big[\\mbox{logit}(\\pi_i)\\big]}{1 + \\exp\\big[\\mbox{logit}(\\pi_i)\\big]}$ where $\\mbox{logit}(\\pi_i)= \\log\\left(\\frac{\\pi_i}{1 - \\pi_i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b817b42d-c459-4706-8077-3ede23ec4499",
   "metadata": {},
   "source": [
    "### What is LASSO regression?\n",
    "\n",
    "- The Least Absolute Shrinkage and Selection Operator(LASSO) regression is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the resulting statistical model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4c1213-0df4-408f-a7c5-ab30e1ee2e13",
   "metadata": {},
   "source": [
    "### What is Support Vector Machine (SVM)?\n",
    "\n",
    "- Supervised learning model\n",
    "- Keys: support vectors, decision boundaries, kernels and basis functions\n",
    "- In SVM algorithm (applicable for both regression and classification), we plot raw data as points in an n-dimensional space where n is the number of features that we have, and the value of each feature is then tied to a particular coordinate on the higher dimensionality, making it easier to classify the data.\n",
    "- Kernel trick: a non Linear data is projected onto a higher dimension space so as to make it easier to classify the data where it could be linearly divided by a plane. This is mathematically achieved by Lagrangian formula using Lagrangian multipliers\n",
    "- Pros:\n",
    "    - SVMs (especially with radial basis functions as kernels) are powerful models, allowing for complex decision boundaries.\n",
    "    - Work well on low dimensional as well as high dimensional data.\n",
    "    - Work well on sparse data.\n",
    "- Cons:\n",
    "    - Computationally expensive and do not scale well on large dataset.\n",
    "    - Require careful preprocessing of data and tuning of hyperparameters.\n",
    "    - Hard to interpret; unlike regression models, we cannot easily interpret the result of SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58dbd52e-2a90-48c8-a448-04c9232ee487",
   "metadata": {},
   "source": [
    "### What is neural network?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8182e6c6-2c56-4a18-bc55-85732f8023ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ce23fa6-80f7-4cb2-9d1a-b877957c2074",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "929b18f2-1248-423b-9514-b27000454367",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e09375fb-e82b-4bd9-be71-19f4030d9a5a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9659ab85-68a0-45ba-a081-e9b2f8f013a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "194ceefc-c7ce-4240-8dff-2aef478a9e7c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dstools]",
   "language": "python",
   "name": "conda-env-dstools-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
